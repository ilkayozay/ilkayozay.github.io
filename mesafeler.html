<!DOCTYPE html>
<html lang="tr">
<head>
    <meta charset="utf-8"/>
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"/>
    <meta name="description" content="Veri analizinde kullanılan mesafe ölçüleri hakkında kapsamlı bir rehber"/>
    <meta name="author" content="AI Asistan"/>
    <title>Veri Analizinde Kullanılan Mesafe Ölçüleri: Türleri, Özellikleri ve Uygulamaları</title>
    <link rel="icon" type="image/x-icon" href="assets/favicon.ico"/>
    <!-- Font Awesome icons (free version)-->
    <script src="https://use.fontawesome.com/releases/v6.1.0/js/all.js" crossorigin="anonymous"></script>
    <!-- Google fonts-->
    <link href="https://fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic" rel="stylesheet" type="text/css"/>
    <link href="https://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800" rel="stylesheet" type="text/css"/>
    <!-- Core theme CSS (includes Bootstrap)-->
    <link href="css/styles.css" rel="stylesheet"/>

    <style>
        ul { padding-left: 1rem; }
        p { text-align: justify }
        .distance-measure { margin-bottom: 30px; }
        .distance-measure h3 { color: #007bff; }
        .example { background-color: #f8f9fa; padding: 15px; border-radius: 5px; margin-top: 10px; }
    </style>
</head>
<body>
    <!-- Navigation-->
    <nav class="navbar navbar-expand-lg navbar-light" id="mainNav">
        <div class="container px-4 px-lg-5">
            <div class="collapse navbar-collapse" id="navbarResponsive">
                <ul class="navbar-nav ms-auto py-4 py-lg-0">
                    <li class="nav-item"><a class="nav-link px-lg-3 py-3 py-lg-4" href="index.html">Ana Sayfa</a></li>
                </ul>
            </div>
        </div>
    </nav>
    <!-- Page Header-->
    <header class="masthead" style="background-image: url('/api/placeholder/1200/800')">
        <div class="container position-relative px-4 px-lg-5">
            <div class="row gx-4 gx-lg-5 justify-content-center">
                <div class="col-md-10 col-lg-8 col-xl-7">
                    <div class="post-heading">
                        <h1>Veri Analizinde Kullanılan Mesafe Ölçüleri</h1>
                        <h2 class="subheading">Türleri, Özellikleri ve Hangi Durumda Hangi Mesafe Ölçüsünün Kullanılması Gerektiği</h2>
                        <span class="meta">7 Ekim 2024 tarihinde yayınlandı</span>
                    </div>
                </div>
            </div>
        </div>
    </header>
    <!-- Post Content-->
    <article class="mb-4">
        <div class="container px-4 px-lg-5">
            <div class="row gx-4 gx-lg-5 justify-content-center">
                <div class="col-md-10 col-lg-8 col-xl-8">
                    <p>
                        Veri analizi ve makine öğrenmesi dünyasında, mesafe ölçüleri veri noktaları arasındaki benzerliği veya farklılığı ölçmek için çok önemli bir rol oynar. Bu ölçüler, kümeleme, sınıflandırma ve boyut azaltma gibi çeşitli algoritma ve tekniklerin temelini oluşturur. Bu kapsamlı rehberde, en yaygın mesafe ölçülerini, özelliklerini ve her birinin ne zaman kullanılması gerektiğini gerçek hayattan örneklerle inceleyeceğiz.
                    </p>

                    <div class="distance-measure">
                        <h3>1. Öklid Mesafesi</h3>
                        <p>
                            Öklid mesafesi, en yaygın kullanılan mesafe ölçüsüdür. İki nokta arasındaki düz çizgi mesafesini hesaplar. Bu, günlük hayatta kullandığımız mesafe kavramına en yakın olan ölçüdür.
                        </p>
                        <ul>
                            <li><strong>Formül:</strong> √(Σ(x<sub>i</sub> - y<sub>i</sub>)<sup>2</sup>)</li>
                            <li><strong>Özellikler:</strong> Sezgisel, orijinal mesafeleri korur</li>
                            <li><strong>En iyi kullanım alanı:</strong> Veriler sürekli ve normal dağılımlı olduğunda</li>
                        </ul>
                        <div class="example">
                            <h4>Gerçek Hayattan Örnek:</h4>
                            <p>
                                Bir emlakçı düşünün. Müşterilerine ev önerirken, evlerin konumunu iki boyutlu bir haritada (enlem ve boylam) gösteriyor. Müşterinin istediği konuma en yakın evleri bulmak için Öklid mesafesini kullanabilir. Örneğin, müşteri (40.7128, -74.0060) koordinatlarında bir ev arıyorsa, diğer evlerin bu noktaya olan Öklid mesafesi hesaplanarak en yakın olanlar sıralanabilir.
                            </p>
                        </div>
                    </div>

                    <div class="distance-measure">
                        <h3>2. Manhattan Mesafesi</h3>
                        <p>
                            Manhattan mesafesi, iki nokta arasındaki yatay ve dikey mesafelerin toplamıdır. Adını New York'un Manhattan bölgesindeki sokak düzeninden alır.
                        </p>
                        <ul>
                            <li><strong>Formül:</strong> Σ|x<sub>i</sub> - y<sub>i</sub>|</li>
                            <li><strong>Özellikler:</strong> Öklid mesafesine göre aykırı değerlere daha az duyarlıdır</li>
                            <li><strong>En iyi kullanım alanı:</strong> Özellikler farklı ölçeklerde olduğunda veya aykırı değerler varsa</li>
                        </ul>
                        <div class="example">
                            <h4>Gerçek Hayattan Örnek:</h4>
                            <p>
                                Bir taksi şoförü düşünün. Manhattan'da bir noktadan diğerine gitmek için, sokakların ızgara yapısı nedeniyle doğrudan bir rota izleyemez. Bunun yerine, yatay ve dikey sokakları takip etmek zorundadır. Örneğin, (0,0) noktasından (3,4) noktasına gitmek için toplam 7 birim (3 birim sağa + 4 birim yukarı) hareket etmesi gerekir. Bu, Manhattan mesafesinin gerçek hayattaki bir uygulamasıdır.
                            </p>
                        </div>
                    </div>

                    <div class="distance-measure">
                        <h3>3. Minkowski Mesafesi</h3>
                        <p>
                            Minkowski mesafesi, Öklid ve Manhattan mesafelerinin genelleştirilmiş halidir. Bir parametre (p) kullanarak farklı mesafe ölçüleri arasında geçiş yapabilir.
                        </p>
                        <ul>
                            <li><strong>Formül:</strong> (Σ|x<sub>i</sub> - y<sub>i</sub>|<sup>p</sup>)<sup>1/p</sup></li>
                            <li><strong>Özellikler:</strong> Esnek, farklı mesafe ölçülerini birleştirir</li>
                            <li><strong>En iyi kullanım alanı:</strong> Farklı mesafe ölçülerinin etkisini incelemek istediğinizde</li>
                        </ul>
                        <div class="example">
                            <h4>Gerçek Hayattan Örnek:</h4>
                            <p>
                                Bir e-ticaret şirketi, müşterilerine ürün önerilerinde bulunurken Minkowski mesafesini kullanabilir. Örneğin, kitap önerilerinde p=2 (Öklid mesafesi) kullanarak genel benzerliği ölçebilir, giysi önerilerinde ise p=1 (Manhattan mesafesi) kullanarak stil farklılıklarına daha duyarlı olabilir. Bu şekilde, farklı ürün kategorileri için en uygun mesafe ölçüsünü seçebilir.
                            </p>
                        </div>
                    </div>

                    <div class="distance-measure">
                        <h3>4. Cosine Mesafesi</h3>
                        <p>
                            Cosine mesafesi, iki vektör arasındaki açının kosinüsünü hesaplar. Bu ölçü, vektörlerin yönünü dikkate alır, büyüklüklerini değil.
                        </p>
                        <ul>
                            <li><strong>Formül:</strong> 1 - (A · B) / (||A|| ||B||)</li>
                            <li><strong>Özellikler:</strong> Ölçekten bağımsız, yüksek boyutlu verilerde kullanışlı</li>
                            <li><strong>En iyi kullanım alanı:</strong> Metin verisi veya yüksek boyutlu seyrek verilerle çalışırken</li>
                        </ul>
                        <div class="example">
                            <h4>Gerçek Hayattan Örnek:</h4>
                            <p>
                                Bir belge sınıflandırma sistemi düşünün. Her belge, içerdiği kelimelerin frekanslarıyla temsil edilir. İki belgenin benzerliğini ölçmek için cosine mesafesi kullanılabilir. Örneğin, bir haber makalesi ile bir bilimsel makale arasındaki benzerlik, kelime vektörlerinin cosine mesafesi hesaplanarak bulunabilir. Bu, belgelerin uzunluğundan bağımsız olarak, içerik benzerliğini ölçer.
                            </p>
                        </div>
                    </div>

                    <div class="distance-measure">
                        <h3>5. Jaccard Mesafesi</h3>
                        <p>
                            Jaccard mesafesi, iki küme arasındaki benzerlik derecesini ölçer. Özellikle ikili (binary) veriler için kullanışlıdır.
                        </p>
                        <ul>
                            <li><strong>Formül:</strong> 1 - |A ∩ B| / |A ∪ B|</li>
                            <li><strong>Özellikler:</strong> İkili veriler için uygun, 0-0 eşleşmelerini göz ardı eder</li>
                            <li><strong>En iyi kullanım alanı:</strong> İkili veriler veya küme tabanlı problemlerle çalışırken</li>
                        </ul>
                        <div class="example">
                            <h4>Gerçek Hayattan Örnek:</h4>
                            <p>
                                Bir film öneri sistemi düşünün. Her kullanıcı, izlediği filmlerin bir listesine sahip. İki kullanıcının film zevklerinin benzerliğini ölçmek için Jaccard mesafesi kullanılabilir. Örneğin, A kullanıcısı {Titanic, Avatar, Star Wars} filmlerini izlemiş, B kullanıcısı ise {Avatar, Star Wars, Inception} filmlerini izlemiş olsun. Jaccard benzerliği (1 - Jaccard mesafesi) = |{Avatar, Star Wars}| / |{Titanic, Avatar, Star Wars, Inception}| = 2/4 = 0.5 olacaktır. Bu, iki kullanıcının film tercihlerinin orta derecede benzer olduğunu gösterir.
                            </p>
                        </div>
                    </div>


         <div class="distance-measure">
                        <h3>6. Mahalanobis Mesafesi</h3>
                        <p>
                            Mahalanobis mesafesi, veri setindeki korelasyonları dikkate alır ve ölçekten bağımsızdır.
                        </p>
                        <ul>
                            <li><strong>Formül:</strong> √((x-μ)<sup>T</sup> S<sup>-1</sup> (x-μ))</li>
                            <li><strong>Özellikler:</strong> Kovaryansı hesaba katar, çok değişkenli verilerde aykırı değerleri tespit eder</li>
                            <li><strong>En iyi kullanım alanı:</strong> Veriler farklı ölçeklerde ve değişkenler arasında korelasyonlar olduğunda</li>
                        </ul>
                        <div class="example">
                            <h4>Gerçek Hayattan Örnek:</h4>
                            <p>
                                Bir kredi skorlama sistemi düşünün. Müşterilerin gelir, harcama, borç gibi finansal özellikleri var. Bu özellikler arasında korelasyonlar olabilir (örneğin, yüksek gelir genellikle yüksek harcama ile ilişkilidir). Mahalanobis mesafesi, bu korelasyonları dikkate alarak daha doğru bir risk değerlendirmesi yapmanıza olanak tanır.
                            </p>
                            <p>
                                Örneğin, iki müşteri düşünelim:
                            </p>
                            <ul>
                                <li>Müşteri A: Yıllık gelir 100.000 TL, aylık harcama 8.000 TL, toplam borç 50.000 TL</li>
                                <li>Müşteri B: Yıllık gelir 50.000 TL, aylık harcama 7.000 TL, toplam borç 40.000 TL</li>
                            </ul>
                            <p>
                                Öklid mesafesi kullanırsak, bu iki müşteri arasında büyük bir fark görebiliriz. Ancak Mahalanobis mesafesi, gelir ile harcama arasındaki pozitif korelasyonu ve gelir ile borç arasındaki ilişkiyi dikkate alır. Bu durumda, Müşteri B'nin gelire oranla daha yüksek harcama ve borç seviyesine sahip olduğunu ve potansiyel olarak daha riskli olabileceğini gösterebilir.
                            </p>
                            <p>
                                Mahalanobis mesafesi, bu tür çok boyutlu ve korelasyonlu verilerde, "normal" profilden ne kadar uzaklaşıldığını ölçerek, kredi risk skorlamasında daha hassas ve doğru sonuçlar elde etmenizi sağlar. Bu sayede, sadece tek tek değerlere bakmak yerine, müşterinin genel finansal profilini daha iyi anlayabilir ve daha doğru kredi kararları verebilirsiniz.
                            </p>
                        </div>
                    </div>
					
					

<div class="distance-measure">
                        <h3>7. Kullback-Leibler Mesafesi (KL Divergence)</h3>
                        <p>
                            Kullback-Leibler Mesafesi, aslında bir mesafe ölçüsü değil, bir divergence (ıraksama) ölçüsüdür. İki olasılık dağılımı arasındaki farkı ölçer. Genellikle bir dağılımın diğerine ne kadar "uzak" olduğunu belirlemek için kullanılır.
                        </p>
                        <ul>
                            <li><strong>Formül:</strong> D<sub>KL</sub>(P||Q) = Σ P(x) * log(P(x) / Q(x))</li>
                            <li><strong>Özellikler:</strong> Asimetrik (P'den Q'ya olan mesafe, Q'dan P'ye olan mesafeden farklı olabilir), negatif olmayan bir değerdir</li>
                            <li><strong>En iyi kullanım alanı:</strong> İki olasılık dağılımını karşılaştırırken, özellikle makine öğrenmesi ve bilgi teorisi uygulamalarında</li>
                        </ul>
                        <div class="example">
                            <h4>Gerçek Hayattan Örnek:</h4>
                            <p>
                                Bir doğal dil işleme uygulaması düşünün. Bu uygulama, farklı yazarların metinlerini analiz ediyor ve bir metnin hangi yazara ait olduğunu tahmin etmeye çalışıyor. Her yazar için, kullandıkları kelimelerin frekans dağılımı bir model olarak oluşturulur. Yeni bir metin geldiğinde, bu metnin kelime dağılımı ile her yazarın modelinin kelime dağılımı arasındaki Kullback-Leibler Mesafesi hesaplanır.
                            </p>
                            <p>
                                Örneğin, elimizde A ve B yazarlarının modelleri var. Yeni gelen bir X metninin A yazarına mı yoksa B yazarına mı ait olduğunu belirlemek istiyoruz:
                            </p>
                            <ul>
                                <li>D<sub>KL</sub>(X||A) = 0.3 (X'in dağılımı ile A'nın dağılımı arasındaki KL mesafesi)</li>
                                <li>D<sub>KL</sub>(X||B) = 0.7 (X'in dağılımı ile B'nin dağılımı arasındaki KL mesafesi)</li>
                            </ul>
                            <p>
                                Bu durumda, X metni A yazarının modeline daha "yakın" olduğu için (daha düşük KL mesafesi), metnin A yazarına ait olma olasılığı daha yüksektir diyebiliriz.
                            </p>
                            <p>
                                KL Divergence'ın asimetrik doğası nedeniyle, D<sub>KL</sub>(A||X) ≠ D<sub>KL</sub>(X||A) olabilir. Bu özellik, özellikle anomali tespiti gibi uygulamalarda faydalı olabilir, çünkü normal ve anormal dağılımlar arasındaki farkı daha belirgin hale getirebilir.
                            </p>
                        </div>
                    </div>

                    <!-- ... (Sonuç bölümü ve diğer içerikler devam edebilir) ... -->
                </div>
            </div>
        </div>
    </article>
    <!-- ... (Footer ve diğer kapanış etiketleri aynı kalacak) ... -->
</body>
</html>